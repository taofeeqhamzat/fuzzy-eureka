{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "\n",
    "# Logistic Regression with Python\n",
    "\n",
    "\n",
    "Estimated time needed: **30** minutes\n",
    "    \n",
    "\n",
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "* Use Logistic Regression for classification\n",
    "* Preprocess data for modeling\n",
    "* Implement Logistic regression on real world data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import the required libraries\n",
    "Make sure the required libraries are available by executing the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==2.2.0\n",
      "  Downloading numpy-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Downloading numpy-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.1\n",
      "    Uninstalling numpy-2.3.1:\n",
      "      Successfully uninstalled numpy-2.3.1\n",
      "Successfully installed numpy-2.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting pandas==2.2.3\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pandas==2.2.3) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas==2.2.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas==2.2.3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas==2.2.3) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.3) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.3.1\n",
      "    Uninstalling pandas-2.3.1:\n",
      "      Successfully uninstalled pandas-2.3.1\n",
      "Successfully installed pandas-2.2.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn==1.6.0\n",
      "  Downloading scikit_learn-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from scikit-learn==1.6.0) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn==1.6.0) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn==1.6.0) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn==1.6.0) (3.6.0)\n",
      "Downloading scikit_learn-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.7.0\n",
      "    Uninstalling scikit-learn-1.7.0:\n",
      "      Successfully uninstalled scikit-learn-1.7.0\n",
      "Successfully installed scikit-learn-1.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting matplotlib==3.9.3\n",
      "  Downloading matplotlib-3.9.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.9.3) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.9.3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.9.3) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.9.3) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from matplotlib==3.9.3) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.9.3) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.9.3) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.9.3) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib==3.9.3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.9.3) (1.17.0)\n",
      "Downloading matplotlib-3.9.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.10.3\n",
      "    Uninstalling matplotlib-3.10.3:\n",
      "      Successfully uninstalled matplotlib-3.10.3\n",
      "Successfully installed matplotlib-3.9.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==2.2.0\n",
    "!pip install pandas==2.2.3\n",
    "!pip install scikit-learn==1.6.0\n",
    "!pip install matplotlib==3.9.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Logistic Regression\n",
    "\n",
    "### Scenario\n",
    "Assume that you are working for a telecommunications company which is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is more likely to leave the company.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load the Telco Churn data \n",
    "Telco Churn is a hypothetical data file that concerns a telecommunications company's efforts to reduce turnover in its customer base. Each case corresponds to a separate customer and it records various demographic and service usage information. Before you can work with the data, you must use the URL to get the ChurnData.csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "We will use a telecommunications dataset for predicting customer churn. This is a historical customer dataset where each row represents one customer. The data is relatively easy to understand, and you may uncover insights you can use immediately. Typically it is less expensive to keep customers than acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company. \n",
    "<br><br>\n",
    "This data set provides you information about customer preferences, services opted, personal details, etc. which helps you predict customer churn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>wireless</th>\n",
       "      <th>longmon</th>\n",
       "      <th>...</th>\n",
       "      <th>pager</th>\n",
       "      <th>internet</th>\n",
       "      <th>callwait</th>\n",
       "      <th>confer</th>\n",
       "      <th>ebill</th>\n",
       "      <th>loglong</th>\n",
       "      <th>logtoll</th>\n",
       "      <th>lninc</th>\n",
       "      <th>custcat</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>6.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.348</td>\n",
       "      <td>3.168</td>\n",
       "      <td>3.850</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.632</td>\n",
       "      <td>3.240</td>\n",
       "      <td>4.357</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>61.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.221</td>\n",
       "      <td>3.664</td>\n",
       "      <td>3.526</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>64.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.708</td>\n",
       "      <td>3.240</td>\n",
       "      <td>4.644</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>28.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.800</td>\n",
       "      <td>3.229</td>\n",
       "      <td>4.234</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>46.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.393</td>\n",
       "      <td>3.240</td>\n",
       "      <td>4.564</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>36.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.140</td>\n",
       "      <td>3.240</td>\n",
       "      <td>4.382</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>59.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.538</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.714</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>46.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.657</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.951</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.70</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.163</td>\n",
       "      <td>3.866</td>\n",
       "      <td>3.219</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>72.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.619</td>\n",
       "      <td>3.240</td>\n",
       "      <td>2.639</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>56.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.209</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.892</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>55.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.854</td>\n",
       "      <td>3.199</td>\n",
       "      <td>4.419</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>5.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.082</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.829</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.758</td>\n",
       "      <td>3.240</td>\n",
       "      <td>4.875</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>52.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.506</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.970</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>14.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.643</td>\n",
       "      <td>3.240</td>\n",
       "      <td>4.205</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916</td>\n",
       "      <td>3.240</td>\n",
       "      <td>4.812</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>37.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.015</td>\n",
       "      <td>3.240</td>\n",
       "      <td>4.762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>28.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.792</td>\n",
       "      <td>3.314</td>\n",
       "      <td>4.159</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>17.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.110</td>\n",
       "      <td>3.199</td>\n",
       "      <td>3.434</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>16.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.322</td>\n",
       "      <td>2.833</td>\n",
       "      <td>5.572</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>71.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.486</td>\n",
       "      <td>3.240</td>\n",
       "      <td>4.290</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>57.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.779</td>\n",
       "      <td>2.639</td>\n",
       "      <td>2.639</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>71.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.431</td>\n",
       "      <td>3.240</td>\n",
       "      <td>5.663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.960</td>\n",
       "      <td>3.240</td>\n",
       "      <td>4.382</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.224</td>\n",
       "      <td>3.168</td>\n",
       "      <td>5.081</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>69.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.256</td>\n",
       "      <td>3.240</td>\n",
       "      <td>2.944</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
       "197     6.0  32.0     10.0    47.0  1.0    10.0    0.0       1.0       0.0   \n",
       "109    35.0  34.0      7.0    78.0  4.0    10.0    0.0       1.0       0.0   \n",
       "70     61.0  43.0      6.0    34.0  5.0     6.0    1.0       1.0       1.0   \n",
       "31     64.0  55.0     28.0   104.0  1.0    26.0    0.0       1.0       0.0   \n",
       "93     28.0  36.0      3.0    69.0  3.0     2.0    1.0       1.0       1.0   \n",
       "75     46.0  45.0     12.0    96.0  3.0    17.0    1.0       1.0       0.0   \n",
       "50     36.0  58.0     34.0    80.0  1.0    21.0    0.0       1.0       0.0   \n",
       "171    59.0  26.0      3.0    41.0  4.0     1.0    1.0       1.0       1.0   \n",
       "38     46.0  42.0      9.0    52.0  4.0     7.0    0.0       1.0       0.0   \n",
       "198    24.0  30.0      0.0    25.0  4.0     5.0    0.0       1.0       1.0   \n",
       "125    72.0  75.0     48.0    14.0  2.0     6.0    0.0       1.0       0.0   \n",
       "10     56.0  52.0     28.0    49.0  2.0    12.0    0.0       1.0       0.0   \n",
       "195    55.0  44.0     24.0    83.0  1.0    23.0    0.0       1.0       0.0   \n",
       "111     5.0  47.0      7.0    46.0  1.0     6.0    0.0       1.0       0.0   \n",
       "25     17.0  42.0      6.0   131.0  5.0     6.0    1.0       0.0       1.0   \n",
       "17     52.0  61.0      3.0    53.0  5.0     1.0    1.0       1.0       1.0   \n",
       "59     14.0  36.0     13.0    67.0  5.0     4.0    1.0       0.0       0.0   \n",
       "29      4.0  47.0      5.0   123.0  4.0    11.0    1.0       1.0       0.0   \n",
       "66     37.0  76.0     38.0   117.0  4.0    21.0    0.0       1.0       0.0   \n",
       "53     28.0  40.0      7.0    64.0  1.0    19.0    0.0       0.0       0.0   \n",
       "113    17.0  42.0      6.0    31.0  2.0     2.0    0.0       0.0       1.0   \n",
       "112    16.0  50.0      5.0   263.0  2.0    29.0    1.0       0.0       1.0   \n",
       "116    71.0  41.0     10.0    73.0  2.0    23.0    0.0       1.0       0.0   \n",
       "194    57.0  60.0     20.0    14.0  2.0    27.0    0.0       1.0       0.0   \n",
       "190    71.0  48.0     25.0   288.0  3.0    19.0    0.0       1.0       0.0   \n",
       "16     27.0  51.0      3.0    80.0  5.0    11.0    1.0       0.0       0.0   \n",
       "15      4.0  35.0     16.0   161.0  5.0     6.0    1.0       0.0       1.0   \n",
       "158    69.0  42.0     23.0    19.0  3.0     0.0    1.0       1.0       0.0   \n",
       "\n",
       "     longmon  ...  pager  internet  callwait  confer  ebill  loglong  logtoll  \\\n",
       "197     3.85  ...    0.0       0.0       1.0     1.0    0.0    1.348    3.168   \n",
       "109    13.90  ...    0.0       0.0       0.0     0.0    0.0    2.632    3.240   \n",
       "70     25.05  ...    1.0       1.0       1.0     1.0    1.0    3.221    3.664   \n",
       "31     15.00  ...    0.0       0.0       0.0     1.0    0.0    2.708    3.240   \n",
       "93      6.05  ...    1.0       0.0       1.0     1.0    1.0    1.800    3.229   \n",
       "75     10.95  ...    0.0       1.0       0.0     0.0    1.0    2.393    3.240   \n",
       "50      8.50  ...    0.0       0.0       1.0     0.0    0.0    2.140    3.240   \n",
       "171    12.65  ...    0.0       1.0       0.0     0.0    0.0    2.538    3.240   \n",
       "38     14.25  ...    0.0       0.0       0.0     0.0    0.0    2.657    3.240   \n",
       "198     8.70  ...    1.0       1.0       1.0     1.0    1.0    2.163    3.866   \n",
       "125    37.30  ...    0.0       0.0       0.0     0.0    0.0    3.619    3.240   \n",
       "10     24.75  ...    0.0       0.0       0.0     0.0    0.0    3.209    3.240   \n",
       "195    17.35  ...    0.0       0.0       0.0     1.0    0.0    2.854    3.199   \n",
       "111     2.95  ...    0.0       0.0       0.0     0.0    0.0    1.082    3.240   \n",
       "25      5.80  ...    0.0       1.0       0.0     0.0    1.0    1.758    3.240   \n",
       "17     12.25  ...    0.0       1.0       0.0     1.0    1.0    2.506    3.240   \n",
       "59     14.05  ...    0.0       1.0       1.0     0.0    0.0    2.643    3.240   \n",
       "29      2.50  ...    0.0       1.0       0.0     0.0    1.0    0.916    3.240   \n",
       "66      7.50  ...    0.0       0.0       0.0     0.0    0.0    2.015    3.240   \n",
       "53      6.00  ...    0.0       0.0       0.0     1.0    0.0    1.792    3.314   \n",
       "113     8.25  ...    0.0       0.0       1.0     1.0    1.0    2.110    3.199   \n",
       "112     3.75  ...    0.0       0.0       1.0     1.0    1.0    1.322    2.833   \n",
       "116    32.65  ...    0.0       0.0       1.0     0.0    0.0    3.486    3.240   \n",
       "194    16.10  ...    0.0       0.0       1.0     1.0    0.0    2.779    2.639   \n",
       "190    30.90  ...    0.0       0.0       0.0     0.0    0.0    3.431    3.240   \n",
       "16      7.10  ...    1.0       1.0       0.0     0.0    1.0    1.960    3.240   \n",
       "15      3.40  ...    1.0       1.0       1.0     1.0    1.0    1.224    3.168   \n",
       "158    25.95  ...    0.0       1.0       0.0     0.0    1.0    3.256    3.240   \n",
       "\n",
       "     lninc  custcat  churn  \n",
       "197  3.850      3.0    0.0  \n",
       "109  4.357      1.0    0.0  \n",
       "70   3.526      4.0    1.0  \n",
       "31   4.644      3.0    0.0  \n",
       "93   4.234      4.0    0.0  \n",
       "75   4.564      2.0    0.0  \n",
       "50   4.382      2.0    0.0  \n",
       "171  3.714      2.0    1.0  \n",
       "38   3.951      2.0    0.0  \n",
       "198  3.219      4.0    1.0  \n",
       "125  2.639      2.0    0.0  \n",
       "10   3.892      2.0    0.0  \n",
       "195  4.419      3.0    0.0  \n",
       "111  3.829      1.0    1.0  \n",
       "25   4.875      2.0    1.0  \n",
       "17   3.970      2.0    0.0  \n",
       "59   4.205      1.0    0.0  \n",
       "29   4.812      1.0    0.0  \n",
       "66   4.762      1.0    0.0  \n",
       "53   4.159      3.0    0.0  \n",
       "113  3.434      4.0    1.0  \n",
       "112  5.572      3.0    0.0  \n",
       "116  4.290      3.0    0.0  \n",
       "194  2.639      3.0    0.0  \n",
       "190  5.663      1.0    0.0  \n",
       "16   4.382      2.0    0.0  \n",
       "15   5.081      4.0    1.0  \n",
       "158  2.944      1.0    0.0  \n",
       "\n",
       "[28 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# churn_df = pd.read_csv(\"ChurnData.csv\")\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/ChurnData.csv\"\n",
    "localURL = \"../data/ChurnData.csv\"\n",
    "churn_df = pd.read_csv(url)\n",
    "\n",
    "churn_df.sample(28)\n",
    "# churn_df.sample(28, axis=1)\n",
    "# churn_df.sample(28, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select some features for the modeling. Also, we change the target data type to be an integer, as it is a requirement by the scikit-learn algorithm:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we can use a subset of the fields available to develop out model. Let us assume that the fields we use are 'tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip' and of course 'churn'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>55.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>34.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>6.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>61.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tenure   age  address  income   ed  employ  equip  callcard  churn\n",
       "0      11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0      1\n",
       "1      33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0      1\n",
       "2      23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0      0\n",
       "3      38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0      0\n",
       "4       7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0      0\n",
       "..      ...   ...      ...     ...  ...     ...    ...       ...    ...\n",
       "195    55.0  44.0     24.0    83.0  1.0    23.0    0.0       1.0      0\n",
       "196    34.0  23.0      3.0    24.0  1.0     7.0    0.0       1.0      0\n",
       "197     6.0  32.0     10.0    47.0  1.0    10.0    0.0       1.0      0\n",
       "198    24.0  30.0      0.0    25.0  4.0     5.0    0.0       1.0      1\n",
       "199    61.0  50.0     16.0   190.0  2.0    22.0    1.0       1.0      0\n",
       "\n",
       "[200 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip', 'callcard', 'churn']]\n",
    "churn_df['churn'] = churn_df['churn'].astype('int')\n",
    "churn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For modeling the input fields X and the target field y need to be fixed. Since that the target to be predicted is 'churn', the data under this field will be stored under the variable 'y'. We may use any combination or all of the remaining fields as the input. Store these values in the variable 'X'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.,  33.,   7., 136.,   5.,   5.,   0.,   1.],\n",
       "       [ 33.,  33.,  12.,  33.,   2.,   0.,   0.,   0.],\n",
       "       [ 23.,  30.,   9.,  30.,   1.,   2.,   0.,   0.],\n",
       "       [ 38.,  35.,   5.,  76.,   2.,  10.,   1.,   1.],\n",
       "       [  7.,  35.,  14.,  80.,   2.,  15.,   0.,   1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip', 'callcard']])\n",
    "X[0:5]  #print the first 5 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.asarray(churn_df['churn'])\n",
    "y[0:5] #print the first 5 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also a norm to standardize or normalize the dataset in order to have all the features at the same scale. This helps the model learn faster and improves the model performance. We may make use of StandardScalar function in the Scikit-Learn library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.13518441, -0.62595491, -0.4588971 ,  0.4751423 ,  1.6961288 ,\n",
       "        -0.58477841, -0.85972695,  0.64686916],\n",
       "       [-0.11604313, -0.62595491,  0.03454064, -0.32886061, -0.6433592 ,\n",
       "        -1.14437497, -0.85972695, -1.54590766],\n",
       "       [-0.57928917, -0.85594447, -0.261522  , -0.35227817, -1.42318853,\n",
       "        -0.92053635, -0.85972695, -1.54590766],\n",
       "       [ 0.11557989, -0.47262854, -0.65627219,  0.00679109, -0.6433592 ,\n",
       "        -0.02518185,  1.16316   ,  0.64686916],\n",
       "       [-1.32048283, -0.47262854,  0.23191574,  0.03801451, -0.6433592 ,\n",
       "         0.53441472, -0.85972695,  0.64686916]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm = StandardScaler().fit(X).transform(X)\n",
    "X_norm[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model has to be tested and evaluated on data which has not been used during training. Therefore, it is required to separate a part of the data for testing and the remaining for training. For this, we may make use of the train_test_split function in the scikit-learn library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X_norm, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model using __LogisticRegression__ from the Scikit-learn package and fit our model with train data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting, or in simple terms training, gives us a model that has now learnt from the traning data and can be used to predict the output variable. Let us predict the churn parameter for the test data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = LR.predict(X_test)\n",
    "yhat[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this prediction, we can also have a look at the prediction probability of data point of the test data set. Use the function __predict_proba__ , we can get the probability of each class. The first column is the probability of the record belonging to class 0, and second column that of class 1. Note that the class prediction system uses the threshold for class prediction as 0.5. This means that the class predicted is the one which is most likely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77047565, 0.22952435],\n",
       "       [0.93543984, 0.06456016],\n",
       "       [0.75062776, 0.24937224],\n",
       "       [0.94883131, 0.05116869],\n",
       "       [0.77117014, 0.22882986],\n",
       "       [0.80793559, 0.19206441],\n",
       "       [0.67970438, 0.32029562],\n",
       "       [0.91850805, 0.08149195],\n",
       "       [0.20574314, 0.79425686],\n",
       "       [0.95243863, 0.04756137]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_prob = LR.predict_proba(X_test)\n",
    "yhat_prob[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the purpose here is to predict the 1 class more acccurately, you can also examine what role each input feature has to play in the prediction of the 1 class. Consider the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAAHHCAYAAACr0swBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAA9hAAAPYQGoP6dpAABUfUlEQVR4nO3dd1gU1/s28HtBWRBYsKCAQZqIoCAEewNb1KjRoMEuNtQEG7ZIrERl1cTYu1E0akiMJX6DvRBj7wYVCwiixhILIKIg7Hn/yMv8XBiUXvT+XNdeujNnzjxndllupq1CCCFARERERFp0irsAIiIiopKIIYmIiIhIBkMSERERkQyGJCIiIiIZDElEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYkIwJ49e+Dm5gZ9fX0oFArEx8cDAH766SfUrFkTZcuWhampKQDAy8sLXl5euV6HQqHA9OnTC6zmohAbGwuFQoGQkJDiLiVfCnoc4eHhUCgUCA8PL5D+CAgJCYFCoUBsbGxxl/JW06dPh0KhwOPHj4u7lBKtf//+sLGxydOyef2MLQwMScUg48NA7jFx4sRCWefx48cxffp06Zd/SRQdHY2hQ4fCzs4O+vr6UKlUaNKkCRYuXIiXL18W2nqfPHkCHx8fGBgYYOnSpfjpp59gaGiIa9euoX///rC3t8fq1auxatWqQquhoGzevBkLFiwo7jLeysvLC7Vr1y7uMnJk2bJlhR4QMwJXxkNXVxeVK1dGt27dEBkZWajrpv+Tnp6OdevWwcvLCxUqVIBSqYSNjQ0GDBiAs2fPFnd5eZbxvho8eLDs/EmTJkltGPyyKlPcBXzIvv32W9ja2mpNK6xfHsePH0dQUBD69+8v7REpScLCwvDFF19AqVSiX79+qF27NlJTU3H06FGMHz8eV65cKbSQcubMGTx//hwzZsxA69atpenh4eHQaDRYuHAhqlevLk3ft29fntbz8uVLlClTuD9ymzdvxuXLlzF69OgC6c/a2hovX75E2bJlC6S/4pLXcSxbtgyVKlVC//79taY3b94cL1++hJ6eXoHVOHLkSNSrVw+vX7/G33//jRUrViA8PByXL1+Gubl5ga2npOrbty969OgBpVJZ5Ot++fIlvL29sWfPHjRv3hzffPMNKlSogNjYWPz6669Yv3494uLi8NFHHxV5bQVBX18fW7duxbJly7K8Z3/++Wfo6+vj1atXxVRdycaQVIzat2+PunXrFncZ+fLixQsYGhrmq4+YmBj06NED1tbWOHToECwsLKR5/v7+iIqKQlhYWH5LzdajR48AIEt4zG56Xn8x6uvr52m54qRQKEpl3ZkV9Dh0dHQKfLs0a9YM3bp1k547Ojriyy+/xIYNGzBhwoQCXde7JCcno1y5ckW6Tl1dXejq6hbpOjOMHz8ee/bswfz587P8gTFt2jTMnz+/yGsqiM/WDO3atcPOnTuxe/dudO7cWZp+/PhxxMTEoGvXrti6dWuBrOt9w8NtJdju3bvRrFkzGBoawtjYGB06dMCVK1e02vz999/o37+/dIjK3NwcAwcOxJMnT6Q206dPx/jx4wEAtra20q7V2NjYt56rkfkcmoxj8VevXkWvXr1Qvnx5NG3aVJq/ceNGeHh4wMDAABUqVECPHj1w586dd45z7ty5SEpKwo8//qgVkDJUr14do0aNkp6npaVhxowZsLe3l3aJf/PNN0hJScn1NvTy8oKvry8AoF69elAoFNKx9GnTpgEAzMzMtLaF3PHyV69eYfr06ahRowb09fVhYWEBb29vREdHZ7s9AeDevXsYOHAgqlSpAqVSiVq1amHt2rVabTIOx/z666+YNWsWPvroI+jr66NVq1aIiorSGktYWBhu374tvcZvnhOwePFi1KpVC+XKlUP58uVRt25dbN68WeYV+T9y74/+/fvDyMgI9+7dQ5cuXWBkZAQzMzOMGzcO6enpb+0vN5YtW4ZatWpBqVTC0tIS/v7+soeLly5dCjs7OxgYGKB+/fr466+/srxGcuN48OABBgwYgI8++ghKpRIWFhbo3LmzdE6MjY0Nrly5gj///FPanhl9ZndO0qlTp/Dpp5+ifPnyMDQ0hKurKxYuXJin8Tdr1gwAtN5DQM7eMwBw+/ZtfPbZZzA0NETlypUREBCAvXv3Zqk74/DnuXPn0Lx5c5QrVw7ffPMNACAlJQXTpk1D9erVoVQqYWVlhQkTJmT5Wdu/fz+aNm0KU1NTGBkZwdHRUeojw7vef9mdk5ST90HGGK5evYoWLVqgXLlyqFq1KubOnfvO7Xz37l2sXLkSbdq0kd0Dq6uri3HjxmXZixQfHy/tmTcxMcGAAQOQnJwszS+oz1YbGxt07NgRR48eRf369aGvrw87Ozts2LDhnWPLULVqVTRv3jzLz/umTZvg4uKS7RGMLVu2SJ/plSpVQp8+fXDv3r0s7Xbs2IHatWtDX18ftWvXxvbt22X702g0WLBgAWrVqgV9fX1UqVIFQ4cOxbNnz3I8lqLGPUnFKCEhIcsx4EqVKgH474RhX19ftG3bFnPmzEFycjKWL1+Opk2b4sKFC9Ivv/379+PWrVsYMGAAzM3NpcNSV65cwcmTJ6FQKODt7Y0bN27g559/xvz586V1mJmZ4d9//8113V988QUcHBwQHBwMIQQAYNasWZgyZQp8fHwwePBg/Pvvv1i8eDGaN2+OCxcuvPUQ3//+9z/Y2dmhcePGOVr/4MGDsX79enTr1g1jx47FqVOnoFarERkZqfXDmZNtOGnSJDg6OmLVqlXS4U97e3t06dIFGzZswPbt27F8+XIYGRnB1dVVtp709HR07NgRBw8eRI8ePTBq1Cg8f/4c+/fvx+XLl2Fvby+73MOHD9GwYUMoFAoMHz4cZmZm2L17NwYNGoTExMQsH9izZ8+Gjo4Oxo0bh4SEBMydOxe9e/fGqVOnAPx3bkFCQgLu3r0r/eVrZGQEAFi9ejVGjhyJbt26YdSoUXj16hX+/vtvnDp1Cr169crRds885rZt26JBgwb4/vvvceDAAcybNw/29vb48ssvc91fZtOnT0dQUBBat26NL7/8EtevX8fy5ctx5swZHDt2TDpstnz5cgwfPhzNmjVDQEAAYmNj0aVLF5QvX/6dh0a6du2KK1euYMSIEbCxscGjR4+wf/9+xMXFwcbGBgsWLMCIESNgZGSESZMmAQCqVKmSbX/79+9Hx44dYWFhgVGjRsHc3ByRkZH4448/tEJ+TmWEhfLly0vTcvqeefHiBVq2bIn79+9LtWzevBmHDx+WXdeTJ0/Qvn179OjRA3369EGVKlWg0Wjw2Wef4ejRoxgyZAicnJwQERGB+fPn48aNG9ixYwcA4MqVK+jYsSNcXV3x7bffQqlUIioqCseOHZP6z+v7L6fvAwB49uwZ2rVrB29vb/j4+OC3337D119/DRcXF7Rv3z7bdezevRtpaWno27fvu14SLT4+PrC1tYVarcb58+exZs0aVK5cGXPmzMlVP2+S+2wFgKioKHTr1g2DBg2Cr68v1q5di/79+8PDwwO1atXKUd+9evXCqFGjkJSUBCMjI6SlpWHLli0YM2aM7KG2kJAQDBgwAPXq1YNarcbDhw+xcOFCHDt2TOszfd++fejatSucnZ2hVqvx5MkT6Y+PzIYOHSr1O3LkSMTExGDJkiW4cOFCltezxBBU5NatWycAyD6EEOL58+fC1NRU+Pn5aS334MEDYWJiojU9OTk5S/8///yzACCOHDkiTfvuu+8EABETE6PVNiYmRgAQ69aty9IPADFt2jTp+bRp0wQA0bNnT612sbGxQldXV8yaNUtrekREhChTpkyW6W9KSEgQAETnzp2zbfOmixcvCgBi8ODBWtPHjRsnAIhDhw4JIXK3DTNejzNnzmi1zRjvv//+qzXd09NTeHp6Ss/Xrl0rAIgffvghS70ajUb6f+btOWjQIGFhYSEeP36stUyPHj2EiYmJ9NoePnxYABBOTk4iJSVFardw4UIBQEREREjTOnToIKytrbPU0blzZ1GrVq0s099F7v3h6+srAIhvv/1Wq627u7vw8PB4Z5+enp5vreXRo0dCT09PfPLJJyI9PV2avmTJEgFArF27VgghREpKiqhYsaKoV6+eeP36tdQuJCREANB6jTKP49mzZwKA+O67795aa61atbT6yZDxmhw+fFgIIURaWpqwtbUV1tbW4tmzZ1pt33wPyMnoa+3ateLff/8V//zzj9izZ4+oXr26UCgU4vTp01LbnL5n5s2bJwCIHTt2SG1evnwpatasqVW3EP+9HgDEihUrtPr86aefhI6Ojvjrr7+0pq9YsUIAEMeOHRNCCDF//nzZn5M35eT9l/FzmPEZldP3wZtj2LBhgzQtJSVFmJubi65du751vQEBAQKAuHDhwlvbZcj4XBg4cKDW9M8//1xUrFhRel4Qn61CCGFtbZ3l8/zRo0dCqVSKsWPHvrNeAMLf3188ffpU6OnpiZ9++kkIIURYWJhQKBQiNjY2y2ddamqqqFy5sqhdu7Z4+fKl1Ncff/whAIipU6dK09zc3ISFhYWIj4+Xpu3bt08A0Pos+uuvvwQAsWnTJq369uzZk2V65s/Y4sTDbcVo6dKl2L9/v9YD+O8v0vj4ePTs2ROPHz+WHrq6umjQoIHWX4MGBgbS/1+9eoXHjx+jYcOGAIDz588XSt3Dhg3Ter5t2zZoNBr4+Pho1Wtubg4HB4ds/3oFgMTERACAsbFxjta9a9cuAMCYMWO0po8dOxYApHOXcrMN82vr1q2oVKkSRowYkWWeQqGQXUYIga1bt6JTp04QQmjV2LZtWyQkJGR5/QYMGKB1PlTG4Zhbt269s0ZTU1PcvXsXZ86cyc3Q3irz+6BZs2Y5quVdDhw4gNTUVIwePRo6Ov/3EeXn5weVSiW9xmfPnsWTJ0/g5+endUJ87969tfa+yDEwMICenh7Cw8MLZFf/hQsXEBMTg9GjR2fZa5rdeyCzgQMHwszMDJaWlmjXrh0SEhLw008/oV69egBy957Zs2cPqlatis8++0zqX19fH35+frLrViqVGDBggNa0LVu2wMnJCTVr1tRaV8uWLQFA+hnKGO/vv/8OjUYj239e3n85fR9kMDIyQp8+faTnenp6qF+//jvfk7n9DMog9/5/8uSJ1F9eZO4zg7Ozs/TzDvx3FMDR0TFXP2/ly5dHu3bt8PPPPwP47yKPxo0bw9raOkvbs2fP4tGjR/jqq6+0zr3r0KEDatasKW37+/fv4+LFi/D19YWJiYnUrk2bNnB2dtbqc8uWLTAxMUGbNm203k8eHh4wMjIq0M/kgsTDbcWofv36sidu37x5EwCkD6PMVCqV9P+nT58iKCgIoaGh0onGGRISEgqw2v+T+Yq8mzdvQggBBwcH2fZv24WaMZbnz5/naN23b9+Gjo6O1tVmAGBubg5TU1Pcvn1bqgnI2TbMr+joaDg6OubqyrV///0X8fHxWLVqVbZX7WV+PatVq6b1PCMI5OSX/Ndff40DBw6gfv36qF69Oj755BP06tULTZo0yXHNb9LX14eZmVmWegoicGS8ho6OjlrT9fT0YGdnJ83P+Dfze6FMmTLvvD+LUqnEnDlzMHbsWFSpUgUNGzZEx44d0a9fvzxdSZZx3lB+rk6dOnUqmjVrhqSkJGzfvh2hoaFa4SA375nbt2/D3t4+S0DLvK0yVK1aNcsFCTdv3kRkZGSW1znzurp37441a9Zg8ODBmDhxIlq1agVvb29069ZNqj8v77+cvg8yfPTRR1nGW758efz999/ZrgPI/WdQhrf9POb18yXzZ2t268pYX25/3nr16oW+ffsiLi4OO3bsyPacrey2PQDUrFkTR48e1Won99nv6Oio9YfezZs3kZCQgMqVK8uuM/PnXUnBkFQCZfw19tNPP8l+YL/5y9jHxwfHjx/H+PHj4ebmBiMjI2g0GrRr1y7bv+relN1fuW87AffNvVcZ9SoUCuzevVv26pSM82LkqFQqWFpa4vLly++s9U3v+us8N9uwOGTU16dPH+nE8cwynwOV3ZU/4o1zF7Lj5OSE69ev448//sCePXuky4GnTp2KoKCgXFaffS2lyejRo9GpUyfs2LEDe/fuxZQpU6BWq3Ho0CG4u7sXeT0uLi7SLSi6dOmC5ORk+Pn5oWnTprCyssrTeyanMv9MA/+9R11cXPDDDz/ILmNlZSUte+TIERw+fBhhYWHYs2cPfvnlF7Rs2RL79u2Drq5ugb//5OT156NmzZoAgIiICLi5uRXY+griszWn68qpzz77DEqlEr6+vkhJSYGPj0+uls8PjUaDypUrY9OmTbLzswvjxY0hqQTKONG3cuXKWvftyezZs2c4ePAggoKCMHXqVGl6xl6UN2X3A5vx10/mq0Uy/5X2rnqFELC1tUWNGjVyvFyGjh07YtWqVThx4gQaNWr01rbW1tbQaDS4efMmnJycpOkPHz5EfHy8tOs4p9uwINjb2+PUqVN4/fp1jk88NDMzg7GxMdLT0wu0vreFR0NDQ3Tv3h3du3dHamoqvL29MWvWLAQGBpaoy/wzXsPr16/Dzs5Omp6amoqYmBhpe2W0i4qKQosWLaR2aWlpiI2NzVFgsLe3x9ixYzF27FjcvHkTbm5umDdvHjZu3Agg54fKMt5vly9fLrDXc/bs2di+fTtmzZqFFStW5Oo9Y21tjatXr0IIoTWGN6+GfBd7e3tcunQJrVq1eud20NHRQatWrdCqVSv88MMPCA4OxqRJk3D48GGp1ty+/3L6Psiv9u3bQ1dXFxs3bsz1ydtvUxCfrQXNwMAAXbp0wcaNG9G+fXvpIp7M3tz2mffGX79+XZqf8a/c75zr169rPbe3t8eBAwfQpEmTbMNgScRzkkqgtm3bQqVSITg4GK9fv84yP+OKtIy/LjL/NSF3x+WM+21k/oFVqVSoVKkSjhw5ojV92bJlOa7X29sburq6CAoKylKLEELrdgRyJkyYAENDQwwePBgPHz7MMj86Olq6jPrTTz8FkHWMGX/tdujQAUDOt2FB6Nq1Kx4/fowlS5ZkmZfdX3q6urrSvUnk9qLltT5DQ0PZw6yZXwM9PT04OztDCCG7fYpT69atoaenh0WLFmltvx9//BEJCQnSa1y3bl1UrFgRq1evRlpamtRu06ZN7zwMkZycnOWKHnt7exgbG2td3m5oaJiju9R//PHHsLW1xYIFC7K0z+1f+2/W07VrV4SEhODBgwe5es+0bdsW9+7dw86dO6Vpr169wurVq3O8fh8fH9y7d092mZcvX+LFixcA/jvkn1nGHpmMbZmX919O3wf5ZWVlBT8/P+zbtw+LFy/OMl+j0WDevHm4e/durvotiM/WwjBu3DhMmzYNU6ZMybZN3bp1UblyZaxYsULr52H37t2IjIyUtr2FhQXc3Nywfv16rc+d/fv34+rVq1p9+vj4ID09HTNmzMiyvrS0tBL7bRDck1QCqVQqLF++HH379sXHH3+MHj16wMzMDHFxcQgLC0OTJk2wZMkSqFQqNG/eHHPnzsXr169RtWpV7Nu3DzExMVn69PDwAPDfZeI9evRA2bJl0alTJymczJ49G4MHD0bdunVx5MgR3LhxI8f12tvbY+bMmQgMDJQuwTY2NkZMTAy2b9+OIUOGYNy4cW9dfvPmzejevTucnJy07rh9/PhxbNmyRbrjcZ06deDr64tVq1YhPj4enp6eOH36NNavX48uXbpIexRyug0LQr9+/bBhwwaMGTMGp0+fRrNmzfDixQscOHAAX331ldbN2940e/ZsHD58GA0aNICfnx+cnZ3x9OlTnD9/HgcOHJD95fMuHh4e+OWXXzBmzBjUq1cPRkZG6NSpEz755BOYm5ujSZMmqFKlCiIjI7FkyRJ06NAh1yesFoR///0XM2fOzDLd1tYWvXv3RmBgIIKCgtCuXTt89tlnuH79OpYtW4Z69epJJ+fq6elh+vTpGDFiBFq2bAkfHx/ExsYiJCRE9nycN924cQOtWrWCj48PnJ2dUaZMGWzfvh0PHz5Ejx49pHYeHh5Yvnw5Zs6cierVq6Ny5cqy57np6Ohg+fLl6NSpE9zc3DBgwABYWFjg2rVruHLlCvbu3Zun7TR+/Hj8+uuvWLBgAWbPnp3j98zQoUOxZMkS9OzZE6NGjYKFhQU2bdok7bHJyR6yvn374tdff8WwYcNw+PBhNGnSBOnp6bh27Rp+/fVX7N27F3Xr1sW3336LI0eOoEOHDrC2tsajR4+wbNkyfPTRR9K9fvLy/jMzM8vR+6AgzJs3D9HR0Rg5ciS2bduGjh07onz58oiLi8OWLVtw7do1rfdFTuX3s7Uw1KlTB3Xq1Hlrm7Jly2LOnDkYMGAAPD090bNnT+kWADY2NggICJDaqtVqdOjQAU2bNsXAgQPx9OlT6Z5YSUlJUjtPT08MHToUarUaFy9exCeffIKyZcvi5s2b2LJlCxYuXKh1M9USo2gvpiMhsr/kPLPDhw+Ltm3bChMTE6Gvry/s7e1F//79xdmzZ6U2d+/eFZ9//rkwNTUVJiYm4osvvhD//PNPlktMhRBixowZomrVqkJHR0frUtvk5GQxaNAgYWJiIoyNjYWPj4949OhRtpepZnep79atW0XTpk2FoaGhMDQ0FDVr1hT+/v7i+vXrOdouN27cEH5+fsLGxkbo6ekJY2Nj0aRJE7F48WLx6tUrqd3r169FUFCQsLW1FWXLlhVWVlYiMDBQq01utmF+bwGQsQ0nTZok1WRubi66desmoqOjpTZyr8nDhw+Fv7+/sLKykpZr1aqVWLVqldYYAIgtW7ZoLSt3iXFSUpLo1auXMDU11boEd+XKlaJ58+aiYsWKQqlUCnt7ezF+/HiRkJCQ9YV4xzp8fX2FoaFhlrYZ2+tdMi7Xlnu0atVKardkyRJRs2ZNUbZsWVGlShXx5ZdfZrm8XgghFi1aJKytrYVSqRT169cXx44dEx4eHqJdu3bZjuPx48fC399f1KxZUxgaGgoTExPRoEED8euvv2r1/eDBA9GhQwdhbGysdVuBzLcAyHD06FHRpk0bYWxsLAwNDYWrq6tYvHjxW7dHdq9vBi8vL6FSqaRLrHPynhFCiFu3bokOHToIAwMDYWZmJsaOHSu2bt0qAIiTJ09qvR7ZXZ6fmpoq5syZI2rVqiWUSqUoX7688PDwEEFBQdJ75+DBg6Jz587C0tJS6OnpCUtLS9GzZ09x48YNqZ+cvP8y3wIgQ07eB9mNwdfXV/aWGHLS0tLEmjVrRLNmzYSJiYkoW7assLa2FgMGDNC6PUB2nwty9RfEZ6u1tbXo0KFDluk5vUwe//8WAG+T3fp/+eUX4e7uLpRKpahQoYLo3bu3uHv3bpblt27dKpycnIRSqRTOzs5i27Zt2W77VatWCQ8PD2FgYCCMjY2Fi4uLmDBhgvjnn39yPbaioBAij/uCiYhKII1GAzMzM3h7e+fq8NKHYMGCBQgICMDdu3dRtWrV4i6HqMTjOUlEVGq9evUqyzk/GzZswNOnT7N8dcyH5uXLl1rPX716hZUrV8LBwYEBiSiHeE4SEZVaJ0+eREBAAL744gtUrFgR58+fx48//ojatWvjiy++KO7yipW3tzeqVasGNzc3JCQkYOPGjbh27Vq2l2ATUVYMSURUatnY2MDKygqLFi3C06dPUaFCBfTr1w+zZ8/OcnPED03btm2xZs0abNq0Cenp6XB2dkZoaCi6d+9e3KURlRo8J4mIiIhIBs9JIiIiIpLBkEREREQkg+ck5YBGo8E///wDY2PjHH9NARERERUvIQSeP38OS0tLrS+MzimGpBz4559/pC9zJCIiotLlzp07+Oijj3K9HENSDmTcNv/OnTtQqVTFXA0RERHlRGJiIqysrPL89UsMSTmQcYhNpVIxJBEREZUyeT1VhiduExEREclgSCIiIiKSwZBEREREJIMhiYiIiEgGQxIRERGRDIYkIiIiIhkMSUREREQyGJKIiIiIZDAkEREREcngHbeJ6INhMzGsuEsgomzEzu5Q3CVkwT1JRERERDIYkoiIiIhkMCQRERERyWBIIiIiIpLBkEREREQk470LSf3790eXLl2KuwwiIiIq5d67WwAsXLgQQojiLoOIiIhKufcuJJmYmBR3CURERPQeKNLDbRqNBmq1Gra2tjAwMECdOnXw22+/SfN37dqFGjVqwMDAAC1atEBISAgUCgXi4+MBANOnT4ebm5tWnwsWLICNjY30PPPhNi8vLwwfPhzDhw+HiYkJKlWqhClTpnBvExEREb1Vke5JUqvV2LhxI1asWAEHBwccOXIEffr0gZmZGezs7ODt7Q1/f38MGTIEZ8+exdixYwtkvevXr8egQYNw+vRpnD17FkOGDEG1atXg5+cn2z4lJQUpKSnS88TExAKpg4iIiEqPIgtJKSkpCA4OxoEDB9CoUSMAgJ2dHY4ePYqVK1fCxsYG9vb2mDdvHgDA0dERERERmDNnTr7XbWVlhfnz50OhUEj9zp8/P9uQpFarERQUlO/1EhERUelVZIfboqKikJycjDZt2sDIyEh6bNiwAdHR0YiMjESDBg20lskIU/nVsGFDKBQKrX5v3ryJ9PR02faBgYFISEiQHnfu3CmQOoiIiKj0KLI9SUlJSQCAsLAwVK1aVWueUqnEyJEj39mHjo5OlnOJXr9+XXBFvlGPUqks8H6JiIio9CiykOTs7AylUom4uDh4enpmme/k5ISdO3dqTTt58qTWczMzMzx48ABCCGnP0MWLF9+57lOnTmXp18HBAbq6urkcBREREX0oiiwkGRsbY9y4cQgICIBGo0HTpk2RkJCAY8eOQaVSYdiwYZg3bx7Gjx+PwYMH49y5cwgJCdHqw8vLC//++y/mzp2Lbt26Yc+ePdi9ezdUKtVb1x0XF4cxY8Zg6NChOH/+PBYvXiyd+0REREQkp0hvATBjxgxMmTIFarUaTk5OaNeuHcLCwmBra4tq1aph69at2LFjB+rUqYMVK1YgODhYa3knJycsW7YMS5cuRZ06dXD69GmMGzfunevt168fXr58ifr168Pf3x+jRo3CkCFDCmuYRERE9B5QiBJ8w6Dw8HC0aNECz549g6mpaZ768PLygpubGxYsWJDnOhITE2FiYoKEhIR37rUiopLLZmJYcZdARNmInd2hwPvM7+/v9+6724iIiIgKAkMSERERkYwS/d1tXl5e+f76kPDw8IIphoiIiD4o3JNEREREJIMhiYiIiEhGiT7cRkRUkArj6hkien9xTxIRERGRDIYkIiIiIhkMSUREREQyGJKIiIiIZDAkEREREclgSCIiIiKSwZBEREREJIMhiYiIiEgGQxIRERGRDIYkIiIiIhkMSUREREQyGJKIiIiIZDAkEREREclgSCIiIiKSwZBEREREJIMhiYiIiEgGQxIRERGRDIYkIiIiIhkMSUREREQyGJKIiIiIZDAkEREREclgSCIiIiKSUaa4CyAiKio2E8MKre/Y2R0KrW8iKh7ck0REREQkgyGJiIiISAZDEhEREZEMhiQiIiIiGQxJRERERDI+yJAUHh4OhUKB+Pj44i6FiIiISqgPMiQRERERvQtDEhEREZGM9yIkaTQaqNVq2NrawsDAAHXq1MFvv/0mzd+1axdq1KgBAwMDtGjRArGxscVXLBEREZUK78Udt9VqNTZu3IgVK1bAwcEBR44cQZ8+fWBmZgY7Ozt4e3vD398fQ4YMwdmzZzF27Ni39peSkoKUlBTpeWJiYmEPgYiIiEqYUh+SUlJSEBwcjAMHDqBRo0YAADs7Oxw9ehQrV66EjY0N7O3tMW/ePACAo6MjIiIiMGfOnGz7VKvVCAoKKpL6iYiIqGQq9SEpKioKycnJaNOmjdb01NRUuLu74+XLl2jQoIHWvIwwlZ3AwECMGTNGep6YmAgrK6uCK5qIiIhKvFIfkpKSkgAAYWFhqFq1qtY8pVKJkSNH5rpPpVIJpVJZIPURERFR6VTqQ5KzszOUSiXi4uLg6emZZb6TkxN27typNe3kyZNFVR4RERGVUqU+JBkbG2PcuHEICAiARqNB06ZNkZCQgGPHjkGlUmHYsGGYN28exo8fj8GDB+PcuXMICQkp7rKJiIiohHsvbgEwY8YMTJkyBWq1Gk5OTmjXrh3CwsJga2uLatWqYevWrdixYwfq1KmDFStWIDg4uLhLJiIiohJOIYQQxV1ESZeYmAgTExMkJCRApVIVdzlElEc2E8MKre/Y2R0KrW8iypv8/v5+L/YkERERERU0hiQiIiIiGQxJRERERDIYkoiIiIhkMCQRERERySj190kiIsopXoFGRLnBPUlEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIBkMSERERkQyGJCIiIiIZDElEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIBkMSERERkQyGJCIiIiIZDElEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIRpniLoCIqCDZTAzLdl7s7A5FWAkRlXbck0REREQkgyGJiIiISAZDEhEREZEMhiQiIiIiGQxJRERERDLyFZK8vLwwevToAiqFiIiIqOTI1y0Atm3bhrJlyxZULUREREQlRr5CUoUKFQqqDiIiIqISpcAOt9nY2CA4OBgDBw6EsbExqlWrhlWrVmm1v3v3Lnr27IkKFSrA0NAQdevWxalTp6T5y5cvh729PfT09ODo6IiffvpJa3mFQoGVK1eiY8eOKFeuHJycnHDixAlERUXBy8sLhoaGaNy4MaKjo7WW+/333/Hxxx9DX18fdnZ2CAoKQlpaWn6GTkRERO+5Aj1xe968eahbty4uXLiAr776Cl9++SWuX78OAEhKSoKnpyfu3buHnTt34tKlS5gwYQI0Gg0AYPv27Rg1ahTGjh2Ly5cvY+jQoRgwYAAOHz6stY4ZM2agX79+uHjxImrWrIlevXph6NChCAwMxNmzZyGEwPDhw6X2f/31F/r164dRo0bh6tWrWLlyJUJCQjBr1qyCHDoRERG9ZxRCCJHXhb28vODm5oYFCxbAxsYGzZo1k/b+CCFgbm6OoKAgDBs2DKtWrcK4ceMQGxsre5iuSZMmqFWrltbeJx8fH7x48QJhYf99zYBCocDkyZMxY8YMAMDJkyfRqFEj/Pjjjxg4cCAAIDQ0FAMGDMDLly8BAK1bt0arVq0QGBgo9btx40ZMmDAB//zzj+y4UlJSkJKSIj1PTEyElZUVEhISoFKp8rq5iKgI8GtJiChDYmIiTExM8vz7u0D3JLm6ukr/VygUMDc3x6NHjwAAFy9ehLu7e7bnMUVGRqJJkyZa05o0aYLIyMhs11GlShUAgIuLi9a0V69eITExEQBw6dIlfPvttzAyMpIefn5+uH//PpKTk2VrUavVMDExkR5WVlY53QRERET0nijQL7jNfKWbQqGQDqcZGBgU+DoUCkW20zLWm5SUhKCgIHh7e2fpS19fX3YdgYGBGDNmjPQ8Y08SERERfTgKNCS9jaurK9asWYOnT5/K7k1ycnLCsWPH4OvrK007duwYnJ2d87Xejz/+GNevX0f16tVzvIxSqYRSqczXeomIiKh0K7KQ1LNnTwQHB6NLly5Qq9WwsLDAhQsXYGlpiUaNGmH8+PHw8fGBu7s7Wrdujf/973/Ytm0bDhw4kK/1Tp06FR07dkS1atXQrVs36Ojo4NKlS7h8+TJmzpxZQKMjIiKi902RfS2Jnp4e9u3bh8qVK+PTTz+Fi4sLZs+eDV1dXQBAly5dsHDhQnz//feoVasWVq5ciXXr1sHLyytf623bti3++OMP7Nu3D/Xq1UPDhg0xf/58WFtbF8CoiIiI6H2Vr6vbPhT5PTueiIoOr24jogwl6uo2IiIiovcFQxIRERGRDIYkIiIiIhkMSUREREQyGJKIiIiIZBTZfZKIiIoCr2AjooLCPUlEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIBkMSERERkQyGJCIiIiIZDElEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIBkMSERERkQyGJCIiIiIZDElEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIRpniLoCIPgw2E8OKuwTEzu5Q3CUQUSnCPUlEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIRqGHpNjYWCgUCly8eDHbNuHh4VAoFIiPjy/scoiIiIhyhHuSiIiIiGSUipCUnp4OjUZT3GUQERHRByTXIWnPnj1o2rQpTE1NUbFiRXTs2BHR0dHS/NOnT8Pd3R36+vqoW7cuLly4kKWPXbt2oUaNGjAwMECLFi0QGxurNT8kJASmpqbYuXMnnJ2doVQqERcXh5SUFIwbNw5Vq1aFoaEhGjRogPDwcGm527dvo1OnTihfvjwMDQ1Rq1Yt7Nq1CwDw7Nkz9O7dG2ZmZjAwMICDgwPWrVuX2+ETERHRByLXX0vy4sULjBkzBq6urkhKSsLUqVPx+eef4+LFi0hOTkbHjh3Rpk0bbNy4ETExMRg1apTW8nfu3IG3tzf8/f0xZMgQnD17FmPHjs2ynuTkZMyZMwdr1qxBxYoVUblyZQwfPhxXr15FaGgoLC0tsX37drRr1w4RERFwcHCAv78/UlNTceTIERgaGuLq1aswMjICAEyZMgVXr17F7t27UalSJURFReHly5eyY0xJSUFKSor0PDExMbebiYiIiEq5XIekrl27aj1fu3YtzMzMcPXqVRw/fhwajQY//vgj9PX1UatWLdy9exdffvml1H758uWwt7fHvHnzAACOjo6IiIjAnDlztPp9/fo1li1bhjp16gAA4uLisG7dOsTFxcHS0hIAMG7cOOzZswfr1q1DcHAw4uLi0LVrV7i4uAAA7OzspP7i4uLg7u6OunXrAgBsbGyyHaNarUZQUFBuNw0RERG9R3J9uO3mzZvo2bMn7OzsoFKppLARFxeHyMhIuLq6Ql9fX2rfqFEjreUjIyPRoEEDrWmZ2wCAnp4eXF1dpecRERFIT09HjRo1YGRkJD3+/PNP6XDfyJEjMXPmTDRp0gTTpk3D33//LS3/5ZdfIjQ0FG5ubpgwYQKOHz+e7RgDAwORkJAgPe7cuZPzDURERETvhVzvSerUqROsra2xevVqWFpaQqPRoHbt2khNTS3QwgwMDKBQKKTnSUlJ0NXVxblz56Crq6vVNuOQ2uDBg9G2bVuEhYVh3759UKvVmDdvHkaMGIH27dvj9u3b2LVrF/bv349WrVrB398f33//fZZ1K5VKKJXKAh0PERERlS652pP05MkTXL9+HZMnT0arVq3g5OSEZ8+eSfOdnJzw999/49WrV9K0kydPavXh5OSE06dPa03L3EaOu7s70tPT8ejRI1SvXl3rYW5uLrWzsrLCsGHDsG3bNowdOxarV6+W5pmZmcHX1xcbN27EggULsGrVqtwMn4iIiD4guQpJ5cuXR8WKFbFq1SpERUXh0KFDGDNmjDS/V69eUCgU8PPzw9WrV7Fr164se2qGDRuGmzdvYvz48bh+/To2b96MkJCQd667Ro0a6N27N/r164dt27YhJiYGp0+fhlqtRlhYGABg9OjR2Lt3L2JiYnD+/HkcPnwYTk5OAICpU6fi999/R1RUFK5cuYI//vhDmkdERESUWa5Cko6ODkJDQ3Hu3DnUrl0bAQEB+O6776T5RkZG+N///oeIiAi4u7tj0qRJWU7IrlatGrZu3YodO3agTp06WLFiBYKDg3O0/nXr1qFfv34YO3YsHB0d0aVLF5w5cwbVqlUD8N/9lPz9/eHk5IR27dqhRo0aWLZsGYD/znEKDAyEq6srmjdvDl1dXYSGhuZm+ERERPQBUQghRHEXUdIlJibCxMQECQkJUKlUxV0OUalkMzGsuEtA7OwOxV0CERWh/P7+LhV33CYiIiIqagxJRERERDIYkoiIiIhkMCQRERERycj1zSSJiPKCJ00TUWnDPUlEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIBkMSERERkQyGJCIiIiIZDElEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIBkMSERERkQyGJCIiIiIZDElEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIRpniLoCISj6biWHFXUKBiJ3dobhLIKJShHuSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIBkMSERERkYxSGZJsbGywYMGC4i6DiIiI3mOlMiQRERERFTaGJCIiIiIZ+QpJGo0GarUatra2MDAwQJ06dfDbb78BAMLDw6FQKLB37164u7vDwMAALVu2xKNHj7B79244OTlBpVKhV69eSE5Olvr08vLC8OHDMXz4cJiYmKBSpUqYMmUKhBDZ1hEXF4fOnTvDyMgIKpUKPj4+ePjwIQAgNjYWOjo6OHv2rNYyCxYsgLW1NTQaTX42AREREb2n8hWS1Go1NmzYgBUrVuDKlSsICAhAnz598Oeff0ptpk+fjiVLluD48eO4c+cOfHx8sGDBAmzevBlhYWHYt28fFi9erNXv+vXrUaZMGZw+fRoLFy7EDz/8gDVr1sjWoNFo0LlzZzx9+hR//vkn9u/fj1u3bqF79+4A/jt/qXXr1li3bp3WcuvWrUP//v2ho5N1E6SkpCAxMVHrQURERB+WPH93W0pKCoKDg3HgwAE0atQIAGBnZ4ejR49i5cqVGDJkCABg5syZaNKkCQBg0KBBCAwMRHR0NOzs7AAA3bp1w+HDh/H1119LfVtZWWH+/PlQKBRwdHREREQE5s+fDz8/vyx1HDx4EBEREYiJiYGVlRUAYMOGDahVqxbOnDmDevXqYfDgwRg2bBh++OEHKJVKnD9/HhEREfj9999lx6ZWqxEUFJTXTUNERETvgTzvSYqKikJycjLatGkDIyMj6bFhwwZER0dL7VxdXaX/V6lSBeXKlZMCUsa0R48eafXdsGFDKBQK6XmjRo1w8+ZNpKenZ6kjMjISVlZWUkACAGdnZ5iamiIyMhIA0KVLF+jq6mL79u0AgJCQELRo0QI2NjayYwsMDERCQoL0uHPnTi62DBEREb0P8rwnKSkpCQAQFhaGqlWras1TKpVSUCpbtqw0XaFQaD3PmFbY5wXp6emhX79+WLduHby9vbF582YsXLgw2/ZKpRJKpbJQayIiIqKSLc8hydnZGUqlEnFxcfD09Mwy/829Sbl16tQprecnT56Eg4MDdHV1s7R1cnLCnTt3cOfOHWlv0tWrVxEfHw9nZ2ep3eDBg1G7dm0sW7YMaWlp8Pb2znN9RERE9P7Lc0gyNjbGuHHjEBAQAI1Gg6ZNmyIhIQHHjh2DSqWCtbV1nouKi4vDmDFjMHToUJw/fx6LFy/GvHnzZNu2bt0aLi4u6N27NxYsWIC0tDR89dVX8PT0RN26daV2Tk5OaNiwIb7++msMHDgQBgYGea6PiIiI3n95DkkAMGPGDJiZmUGtVuPWrVswNTXFxx9/jG+++SZfh9D69euHly9fon79+tDV1cWoUaOkE8EzUygU+P333zFixAg0b94cOjo6aNeuXZYr5oD/Thw/fvw4Bg4cmOfaiIiI6MOgEG+7AVEx8PLygpubW6F87ciMGTOwZcsW/P3337laLjExESYmJkhISIBKpSrwuohKOpuJYcVdQoGInd2huEsgoiKU39/fH8Qdt5OSknD58mUsWbIEI0aMKO5yiIiIqBT4IELS8OHD4eHhAS8vLx5qIyIiohzJ1zlJhSE8PLzA+wwJCUFISEiB90tERETvrw9iTxIRERFRbjEkEREREckocYfbiKjk4VVhRPQh4p4kIiIiIhkMSUREREQyGJKIiIiIZDAkEREREclgSCIiIiKSwZBEREREJIMhiYiIiEgGQxIRERGRDIYkIiIiIhkMSUREREQyGJKIiIiIZDAkEREREclgSCIiIiKSwZBEREREJIMhiYiIiEgGQxIRERGRDIYkIiIiIhkMSUREREQyGJKIiIiIZDAkEREREclgSCIiIiKSwZBEREREJKNMcRdARLlnMzGsuEsolWJndyjuEoioFOGeJCIiIiIZDElEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIBkMSERERkYxSEZL27NmDpk2bwtTUFBUrVkTHjh0RHR0tzT9+/Djc3Nygr6+PunXrYseOHVAoFLh48aLU5vLly2jfvj2MjIxQpUoV9O3bF48fPy6G0RAREVFpUCpC0osXLzBmzBicPXsWBw8ehI6ODj7//HNoNBokJiaiU6dOcHFxwfnz5zFjxgx8/fXXWsvHx8ejZcuWcHd3x9mzZ7Fnzx48fPgQPj4+sutLSUlBYmKi1oOIiIg+LKXia0m6du2q9Xzt2rUwMzPD1atXcfToUSgUCqxevRr6+vpwdnbGvXv34OfnJ7VfsmQJ3N3dERwcrNWHlZUVbty4gRo1amj1r1arERQUVLiDIiIiohKtVOxJunnzJnr27Ak7OzuoVCrY2NgAAOLi4nD9+nW4urpCX19fal+/fn2t5S9duoTDhw/DyMhIetSsWRMAtA7bZQgMDERCQoL0uHPnTuENjoiIiEqkUrEnqVOnTrC2tsbq1athaWkJjUaD2rVrIzU1NUfLJyUloVOnTpgzZ06WeRYWFlmmKZVKKJXKfNdNREREpVeJD0lPnjzB9evXsXr1ajRr1gwAcPToUWm+o6MjNm7ciJSUFCnYnDlzRquPjz/+GFu3boWNjQ3KlCnxQyYiIqISoMQfbitfvjwqVqyIVatWISoqCocOHcKYMWOk+b169YJGo8GQIUMQGRmJvXv34vvvvwcAKBQKAIC/vz+ePn2Knj174syZM4iOjsbevXsxYMAApKenF8u4iIiIqGQr8SFJR0cHoaGhOHfuHGrXro2AgAB899130nyVSoX//e9/uHjxItzc3DBp0iRMnToVAKTzlCwtLXHs2DGkp6fjk08+gYuLC0aPHg1TU1Po6JT4TUBERETFoFQce2rdujWuXr2qNU0IIf2/cePGuHTpkvR806ZNKFu2LKpVqyZNc3BwwLZt2wq/WCIiInovlIqQ9C4bNmyAnZ0dqlatikuXLuHrr7+Gj48PDAwMirs0IiIiKqXei5D04MEDTJ06FQ8ePICFhQW++OILzJo1q7jLIiIiolLsvQhJEyZMwIQJE4q7DCIiInqP8KxlIiIiIhkMSUREREQy3ovDbUQfmtjZHYq7BCKi9x73JBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIBkMSERERkQyGJCIiIiIZDElEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIBkMSERERkQyGJCIiIiIZDElEREREMhiSiIiIiGQwJBERERHJYEgiIiIiksGQRERERCSDIYmIiIhIBkMSERERkQyGJCIiIiIZZYq7AKLSzmZiWHGXQDkUO7tDcZdARKUI9yQRERERyWBIIiIiIpLBkEREREQkgyGJiIiISAZDEhEREZGMIg1J06dPh5ubm/S8f//+6NKlS1GWIFEoFNixY0exrJuIiIhKPu5JIiIiIpLxXoek1NTU4i6BiIiISqlchySNRoO5c+eievXqUCqVqFatGmbNmgUA+Prrr1GjRg2UK1cOdnZ2mDJlCl6/fl0gfeek/4zDeWvWrIGtrS309fUBADdv3kTz5s2hr68PZ2dn7N+/P7fDJiIiog9Mru+4HRgYiNWrV2P+/Plo2rQp7t+/j2vXrgEAjI2NERISAktLS0RERMDPzw/GxsaYMGFCvvvOaf9RUVHYunUrtm3bBl1dXWg0Gnh7e6NKlSo4deoUEhISMHr06LfWkZKSgpSUFOl5YmJiLrYQERERvQ9yFZKeP3+OhQsXYsmSJfD19QUA2Nvbo2nTpgCAyZMnS21tbGwwbtw4hIaG5igkvavvnPafmpqKDRs2wMzMDACwb98+XLt2DXv37oWlpSUAIDg4GO3bt8+2FrVajaCgoHfWTERERO+vXIWkyMhIpKSkoFWrVrLzf/nlFyxatAjR0dFISkpCWloaVCpVgfSd0/6tra2lgJTRr5WVlRSQAKBRo0ZvrSUwMBBjxoyRnicmJsLKyipH4yAiIqL3Q67OSTIwMMh23okTJ9C7d298+umn+OOPP3DhwgVMmjQpxydPv63v3PRvaGiYo/W9jVKphEql0noQERHRhyVXIcnBwQEGBgY4ePBglnnHjx+HtbU1Jk2ahLp168LBwQG3b98ukL7z07+TkxPu3LmD+/fvS9NOnjyZ47qIiIjow5Srw236+vr4+uuvMWHCBOjp6aFJkyb4999/ceXKFTg4OCAuLg6hoaGoV68ewsLCsH379gLpe9CgQXnuv3Xr1qhRowZ8fX3x3XffITExEZMmTcrNsImIiOgDlOtbAEyZMgVjx47F1KlT4eTkhO7du+PRo0f47LPPEBAQgOHDh8PNzQ3Hjx/HlClTCqRvAHnuX0dHB9u3b8fLly9Rv359DB48WOu2AkRERERyFEIIUdxFlHSJiYkwMTFBQkICz0+iLGwmhhV3CZRDsbM7FHcJRFSE8vv7+72+4zYRERFRXjEkEREREclgSCIiIiKSwZBEREREJIMhiYiIiEhGrr/gloi08YopIqL3E/ckEREREclgSCIiIiKSwZBEREREJIMhiYiIiEgGQxIRERGRDIYkIiIiIhkMSUREREQyGJKIiIiIZDAkEREREclgSCIiIiKSwZBEREREJIMhiYiIiEgGQxIRERGRDIYkIiIiIhkMSUREREQyGJKIiIiIZDAkEREREclgSCIiIiKSwZBEREREJIMhiYiIiEgGQxIRERGRDIYkIiIiIhllirsAAmwmhhV3CUQfhNjZHYq7BCIqRbgniYiIiEgGQxIRERGRDIYkIiIiIhkMSUREREQyGJKIiIiIZDAkEREREckolJDk5eWF0aNHF0bXREREREXivd6TJIRAWlpacZdBREREpVCBh6T+/fvjzz//xMKFC6FQKKBQKBAbG4vLly+jffv2MDIyQpUqVdC3b188fvxYWs7LywsjR47EhAkTUKFCBZibm2P69OnS/NjYWCgUCly8eFGaFh8fD4VCgfDwcABAeHg4FAoFdu/eDQ8PDyiVShw9ehQajQZqtRq2trYwMDBAnTp18NtvvxX00ImIiOg9UuAhaeHChWjUqBH8/Pxw//593L9/H8bGxmjZsiXc3d1x9uxZ7NmzBw8fPoSPj4/WsuvXr4ehoSFOnTqFuXPn4ttvv8X+/ftzXcPEiRMxe/ZsREZGwtXVFWq1Ghs2bMCKFStw5coVBAQEoE+fPvjzzz8LathERET0ninwryUxMTGBnp4eypUrB3NzcwDAzJkz4e7ujuDgYKnd2rVrYWVlhRs3bqBGjRoAAFdXV0ybNg0A4ODggCVLluDgwYNo06ZNrmr49ttvpWVSUlIQHByMAwcOoFGjRgAAOzs7HD16FCtXroSnp2eW5VNSUpCSkiI9T0xMzNX6iYiIqPQrku9uu3TpEg4fPgwjI6Ms86Kjo7VC0pssLCzw6NGjXK+vbt260v+joqKQnJycJWilpqbC3d1ddnm1Wo2goKBcr5eIiIjeH0USkpKSktCpUyfMmTMnyzwLCwvp/2XLltWap1AooNFoAAA6Ov8dGRRCSPNfv34tuz5DQ0OtdQNAWFgYqlatqtVOqVTKLh8YGIgxY8ZIzxMTE2FlZSXbloiIiN5PhRKS9PT0kJ6eLj3/+OOPsXXrVtjY2KBMmbyt0szMDABw//59aQ/QmydxZ8fZ2RlKpRJxcXGyh9bkKJXKbAMUERERfRgKJSTZ2Njg1KlTiI2NhZGREfz9/bF69Wr07NlTunotKioKoaGhWLNmDXR1dd/Zp4GBARo2bIjZs2fD1tYWjx49wuTJk9+5nLGxMcaNG4eAgABoNBo0bdoUCQkJOHbsGFQqFXx9fQtiyERERPSeKZT7JI0bNw66urpwdnaGmZkZUlNTcezYMaSnp+OTTz6Bi4sLRo8eDVNTU+kwWk6sXbsWaWlp8PDwwOjRozFz5swcLTdjxgxMmTIFarUaTk5OaNeuHcLCwmBra5vXIRIREdF7TiHePMmHZCUmJsLExAQJCQlQqVQF3r/NxLAC75OIsoqd3aG4SyCiIpTf39/v9R23iYiIiPKKIYmIiIhIBkMSERERkQyGJCIiIiIZDElEREREMorkjtv0drzihoiIqOThniQiIiIiGQxJRERERDIYkoiIiIhkMCQRERERyWBIIiIiIpLBkEREREQkgyGJiIiISAZDEhEREZEMhiQiIiIiGQxJRERERDL4tSQ5IIQAACQmJhZzJURERJRTGb+3M36P5xZDUg48f/4cAGBlZVXMlRAREVFuPX/+HCYmJrleTiHyGq8+IBqNBv/88w+MjY2hUCiKu5wCl5iYCCsrK9y5cwcqlaq4yykyHDfH/SHguD+scQMf7tjlxi2EwPPnz2FpaQkdndyfYcQ9STmgo6ODjz76qLjLKHQqleqD+oHKwHF/WDjuD8uHOm7gwx175nHnZQ9SBp64TURERCSDIYmIiIhIBkMSQalUYtq0aVAqlcVdSpHiuDnuDwHH/WGNG/hwx14Y4+aJ20REREQyuCeJiIiISAZDEhEREZEMhiQiIiIiGQxJRERERDIYkj5AT58+Re/evaFSqWBqaopBgwYhKSnpncudOHECLVu2hKGhIVQqFZo3b46XL18WQcUFI6/jBv67a2v79u2hUCiwY8eOwi20EOR27E+fPsWIESPg6OgIAwMDVKtWDSNHjkRCQkIRVp17S5cuhY2NDfT19dGgQQOcPn36re23bNmCmjVrQl9fHy4uLti1a1cRVVqwcjPu1atXo1mzZihfvjzKly+P1q1bv3M7lVS5fb0zhIaGQqFQoEuXLoVbYCHJ7bjj4+Ph7+8PCwsLKJVK1KhR44N4rwPAggULpM8xKysrBAQE4NWrVzlfoaAPTrt27USdOnXEyZMnxV9//SWqV68uevbs+dZljh8/LlQqlVCr1eLy5cvi2rVr4pdffhGvXr0qoqrzLy/jzvDDDz+I9u3bCwBi+/bthVtoIcjt2CMiIoS3t7fYuXOniIqKEgcPHhQODg6ia9euRVh17oSGhgo9PT2xdu1aceXKFeHn5ydMTU3Fw4cPZdsfO3ZM6Orqirlz54qrV6+KyZMni7Jly4qIiIgirjx/cjvuXr16iaVLl4oLFy6IyMhI0b9/f2FiYiLu3r1bxJXnT27HnSEmJkZUrVpVNGvWTHTu3Lloii1AuR13SkqKqFu3rvj000/F0aNHRUxMjAgPDxcXL14s4srzL7dj37Rpk1AqlWLTpk0iJiZG7N27V1hYWIiAgIAcr5Mh6QNz9epVAUCcOXNGmrZ7926hUCjEvXv3sl2uQYMGYvLkyUVRYqHI67iFEOLChQuiatWq4v79+6UyJOVn7G/69ddfhZ6ennj9+nVhlJlv9evXF/7+/tLz9PR0YWlpKdRqtWx7Hx8f0aFDB61pDRo0EEOHDi3UOgtabsedWVpamjA2Nhbr168vrBILRV7GnZaWJho3bizWrFkjfH19S2VIyu24ly9fLuzs7ERqampRlVhocjt2f39/0bJlS61pY8aMEU2aNMnxOnm47QNz4sQJmJqaom7dutK01q1bQ0dHB6dOnZJd5tGjRzh16hQqV66Mxo0bo0qVKvD09MTRo0eLqux8y8u4ASA5ORm9evXC0qVLYW5uXhSlFri8jj2zhIQEqFQqlClT8r7yMTU1FefOnUPr1q2laTo6OmjdujVOnDghu8yJEye02gNA27Zts21fEuVl3JklJyfj9evXqFChQmGVWeDyOu5vv/0WlStXxqBBg4qizAKXl3Hv3LkTjRo1gr+/P6pUqYLatWsjODgY6enpRVV2gcjL2Bs3boxz585Jh+Ru3bqFXbt24dNPP83xekvepx0VqgcPHqBy5cpa08qUKYMKFSrgwYMHssvcunULADB9+nR8//33cHNzw4YNG9CqVStcvnwZDg4OhV53fuVl3AAQEBCAxo0bo3PnzoVdYqHJ69jf9PjxY8yYMQNDhgwpjBLz7fHjx0hPT0eVKlW0plepUgXXrl2TXebBgwey7XO6TUqCvIw7s6+//hqWlpZZAmNJlpdxHz16FD/++CMuXrxYBBUWjryM+9atWzh06BB69+6NXbt2ISoqCl999RVev36NadOmFUXZBSIvY+/VqxceP36Mpk2bQgiBtLQ0DBs2DN98802O18s9Se+JiRMnQqFQvPWR0w/NzDQaDQBg6NChGDBgANzd3TF//nw4Ojpi7dq1BTmMXCvMce/cuROHDh3CggULCrboAlKYY39TYmIiOnToAGdnZ0yfPj3/hVOJMXv2bISGhmL79u3Q19cv7nIKzfPnz9G3b1+sXr0alSpVKu5yipRGo0HlypWxatUqeHh4oHv37pg0aRJWrFhR3KUVuvDwcAQHB2PZsmU4f/48tm3bhrCwMMyYMSPHfXBP0nti7Nix6N+//1vb2NnZwdzcHI8ePdKanpaWhqdPn2Z7OMnCwgIA4OzsrDXdyckJcXFxeS+6ABTmuA8dOoTo6GiYmppqTe/atSuaNWuG8PDwfFSef4U59gzPnz9Hu3btYGxsjO3bt6Ns2bL5LbtQVKpUCbq6unj48KHW9IcPH2Y7RnNz81y1L4nyMu4M33//PWbPno0DBw7A1dW1MMsscLkdd3R0NGJjY9GpUydpWsYff2XKlMH169dhb29fuEUXgLy83hYWFihbtix0dXWlaU5OTnjw4AFSU1Ohp6dXqDUXlLyMfcqUKejbty8GDx4MAHBxccGLFy8wZMgQTJo0CTo6795PxJD0njAzM4OZmdk72zVq1Ajx8fE4d+4cPDw8APwXBjQaDRo0aCC7jI2NDSwtLXH9+nWt6Tdu3ED79u3zX3w+FOa4J06cKP1wZXBxccH8+fO1PmyLS2GOHfhvD1Lbtm2hVCqxc+fOEr2nQU9PDx4eHjh48KB0WbdGo8HBgwcxfPhw2WUaNWqEgwcPYvTo0dK0/fv3o1GjRkVQccHIy7gBYO7cuZg1axb27t2rda5aaZHbcdesWRMRERFa0yZPnoznz59j4cKFsLKyKoqy8y0vr3eTJk2wefNmaDQaKRTcuHEDFhYWpSYgAXkbe3JycpYglBEWRU6/tjYvZ5hT6dauXTvh7u4uTp06JY4ePSocHBy0Lge/e/eucHR0FKdOnZKmzZ8/X6hUKrFlyxZx8+ZNMXnyZKGvry+ioqKKYwh5kpdxZ4ZSeHWbELkfe0JCgmjQoIFwcXERUVFR4v79+9IjLS2tuIbxVqGhoUKpVIqQkBBx9epVMWTIEGFqaioePHgghBCib9++YuLEiVL7Y8eOiTJlyojvv/9eREZGimnTppXaWwDkZtyzZ88Wenp64rffftN6XZ8/f15cQ8iT3I47s9J6dVtuxx0XFyeMjY3F8OHDxfXr18Uff/whKleuLGbOnFlcQ8iz3I592rRpwtjYWPz888/i1q1bYt++fcLe3l74+PjkeJ0MSR+gJ0+eiJ49ewojIyOhUqnEgAEDtD4gY2JiBABx+PBhreXUarX46KOPRLly5USjRo3EX3/9VcSV509ex/2m0hqScjv2w4cPCwCyj5iYmOIZRA4sXrxYVKtWTejp6Yn69euLkydPSvM8PT2Fr6+vVvtff/1V1KhRQ+jp6YlatWqJsLCwIq64YORm3NbW1rKv67Rp04q+8HzK7ev9ptIakoTI/biPHz8uGjRoIJRKpbCzsxOzZs0qsX/svEtuxv769Wsxffp0YW9vL/T19YWVlZX46quvxLNnz3K8PoUQOd3nRERERPTh4NVtRERERDIYkoiIiIhkMCQRERERyWBIIiIiIpLBkEREREQkgyGJiIiISAZDEhEREZEMhiQiKjEePHiANm3awNDQUPrOPLlpCoUCO3bsyFGf06dPh5ubW6HUWxRKe/1EpRlDEhG904MHDzBixAjY2dlBqVTCysoKnTp1wsGDBwt0PfPnz8f9+/dx8eJF3LhxI9tp9+/fz/H3Bo4bN67A6wwJCcnyxceZzZs3D+XLl8erV6+yzEtOToZKpcKiRYsKtC4iKlgMSUT0VrGxsfDw8MChQ4fw3XffISIiAnv27EGLFi3g7+9foOuKjo6Gh4cHHBwcULly5WynmZubQ6lU5qhPIyMjVKxYsUDrzIm+ffvixYsX2LZtW5Z5v/32G1JTU9GnT58ir4uIcqHAvlCFiN5L7du3F1WrVhVJSUlZ5r35HUi3b98Wn332mTA0NBTGxsbiiy++kL54MsOOHTuEu7u7UCqVwtbWVkyfPl28fv1aCJH1O8V8fX1lpwmR9Tv07ty5I3r06CHKly8vypUrJzw8PKTvdJo2bZqoU6eOVh2rV68WNWvWFEqlUjg6OoqlS5dK8zK+x27r1q3Cy8tLGBgYCFdXV3H8+HEhhPz32mX3vWfe3t6iVatWWaZ7enqK7t27CyGEmDBhgnBwcBAGBgbC1tZWTJ48WaSmpkptM9fv6ekpRo0apdVf586dtb6z6tWrV2Ls2LHC0tJSlCtXTtSvX/+t30lIRPLKFE80I6LS4OnTp9izZw9mzZoFQ0PDLPMzDjlpNBp07twZRkZG+PPPP5GWlgZ/f390794d4eHhAIC//voL/fr1w6JFi9CsWTNER0djyJAhAIBp06bhzJkz6NevH1QqFRYuXAgDAwOkpqZmmZZZUlISPD09UbVqVezcuRPm5uY4f/48NBqN7Jg2bdqEqVOnYsmSJXB3d8eFCxfg5+cHQ0ND+Pr6Su0mTZqE77//Hg4ODpg0aRJ69uyJqKgoNG7cGAsWLMDUqVNx/fp1AP/trZIzaNAgdOzYEbdv34a1tTUA4NatWzhy5Aj27t0LADA2NkZISAgsLS0REREBPz8/GBsbY8KECTl4heQNHz4cV69eRWhoKCwtLbF9+3a0a9cOERERcHBwyHO/RB+c4k5pRFRynTp1SgAQ27Zte2u7ffv2CV1dXREXFydNu3LligAgTp8+LYQQolWrViI4OFhruZ9++klYWFhIzzPvEcluGt7Yk7Ry5UphbGwsnjx5Iltb5j0x9vb2YvPmzVptZsyYIRo1aiSE+L89SWvWrMkylsjISCGEEOvWrRMmJibyG+MNaWlpomrVqlp7mqZMmSKqVasm0tPTZZf57rvvhIeHR7b1v2tP0u3bt4Wurq64d++eVptWrVqJwMDAd9ZMRP+He5KIKFtCiBy1i4yMhJWVFaysrKRpzs7OMDU1RWRkJOrVq4dLly7h2LFjmDVrltQmPT0dr169QnJyMsqVK5enGi9evAh3d3dUqFDhnW1fvHiB6OhoDBo0CH5+ftL0tLQ0mJiYaLV1dXWV/m9hYQEAePToEWrWrJnj2nR1deHr64uQkBBMmzYNQgisX78eAwYMgI7Of6eE/vLLL1i0aBGio6ORlJSEtLQ0qFSqHK8js4iICKSnp6NGjRpa01NSUorl3Cyi0owhiYiy5eDgAIVCgWvXruW7r6SkJAQFBcHb2zvLPH19/Tz3K3cI7m01AMDq1avRoEEDrXm6urpaz8uWLSv9X6FQAEC2h/DeZuDAgVCr1Th06BA0Gg3u3LmDAQMGAABOnDiB3r17IygoCG3btoWJiQlCQ0Mxb968bPvT0dHJEl5fv36tNUZdXV2cO3cuy5iyOyxIRPIYkogoWxUqVEDbtm2xdOlSjBw5Mst5SfHx8TA1NYWTkxPu3LmDO3fuSHuTrl69ivj4eDg7OwMAPv74Y1y/fh3Vq1cv0BpdXV2xZs0aPH369J17k6pUqQJLS0vcunULvXv3zvM69fT0kJ6enqO29vb28PT0xNq1ayGEQOvWraXzk44fPw5ra2tMmjRJan/79u239mdmZob79+9Lz9PT03H58mW0aNECAODu7o709HQ8evQIzZo1y+3QiOgNvAUAEb3V0qVLkZ6ejvr162Pr1q24efMmIiMjsWjRIjRq1AgA0Lp1a7i4uKB37944f/48Tp8+jX79+sHT0xN169YFAEydOhUbNmxAUFAQrly5gsjISISGhmLy5Mn5qq9nz54wNzdHly5dcOzYMdy6dQtbt27FiRMnZNsHBQVBrVZj0aJFuHHjBiIiIrBu3Tr88MMPOV6njY0NkpKScPDgQTx+/BjJyclvbT9o0CBs27YN27dvx6BBg6TpDg4OiIuLQ2hoKKKjo7Fo0SJs3779rX21bNkSYWFhCAsLw7Vr1/Dll18iPj5eml+jRg307t0b/fr1w7Zt2xATE4PTp09DrVYjLCwsx2MkIoYkInoHOzs7nD9/Hi1atMDYsWNRu3ZttGnTBgcPHsTy5csB/Hc46vfff0f58uXRvHlztG7dGnZ2dvjll1+kftq2bYs//vgD+/btQ7169dCwYUPMnz9f2quSV3p6eti3bx8qV66MTz/9FC4uLpg9e3aWQ00ZBg8ejDVr1mDdunVwcXGBp6cnQkJCYGtrm+N1Nm7cGMOGDUP37t1hZmaGuXPnvrV9165doVQqUa5cOXTp0kWa/tlnnyEgIADDhw+Hm5sbjh8/jilTpry1r4EDB8LX11cKoXZ2dtJepAzr1q1Dv379MHbsWDg6OqJLly44c+YMqlWrluMxEhGgEDk9M5OIiIjoA8I9SUREREQyGJKIiIiIZDAkEREREclgSCIiIiKSwZBEREREJIMhiYiIiEgGQxIRERGRDIYkIiIiIhkMSUREREQyGJKIiIiIZDAkEREREclgSCIiIiKS8f8AppXtLM22/4IAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coefficients = pd.Series(LR.coef_[0], index=churn_df.columns[:-1])\n",
    "coefficients.sort_values().plot(kind='barh')\n",
    "plt.title(\"Feature Coefficients in Logistic Regression Churn Model\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large positive value of LR Coefficient for a given field indicates that increase in this parameter will lead to better chance of a positive, i.e. 1 class. A large negative value indicates the opposite, which means that an increase in this parameter will lead to poorer chance of a positive class. A lower absolute value indicates weaker affect of the change in that field on the predicted class. Let us examine this with the following exercises.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the predictions have been generated, it becomes prudent to evaluate the performance of the model in predicting the target variable. Let us evaluate the log-loss value.\n",
    "\n",
    "### log loss\n",
    "\n",
    "Log loss (Logarithmic loss), also known as Binary Cross entropy loss, is a function that generates a loss value based on the class wise prediction probabilities and the actual class labels. The lower the log loss value, the better the model is considered to be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6039104035600186"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, yhat_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "Try to attempt the following questions yourself based on what you learnt in this lab.\n",
    "\n",
    "<br>\n",
    "\n",
    "a. Let us assume we add the feature 'callcard' to the original set of input features. What will the value of log loss be in this case?\n",
    "<br>\n",
    "<details><summary>Hint</summary>\n",
    "Reuse all the code statements above after modifying the value of churn_df. Make sure to edit the list of features feeding the variable X. The expected answer is 0.6039104035600186.\n",
    "</details>\n",
    "\n",
    "b. Let us assume we add the feature 'wireless' to the original set of input features. What will the value of log loss be in this case?\n",
    "<br>\n",
    "<details><summary>Hint</summary>\n",
    "Reuse all the code statements above after modifying the value of churn_df. Make sure to edit the list of features feeding the variable X. The expected answer is 0.7227054293985518.\n",
    "</details>\n",
    "\n",
    "c. What happens to the log loss value if we add both \"callcard\" and \"wireless\" to the input features?\n",
    "<br>\n",
    "<details><summary>Hint</summary>\n",
    "Reuse all the code statements above after modifying the value of churn_df. Make sure to edit the list of features feeding the variable X. The expected answer is 0.7760557225417114\n",
    "</details>\n",
    "\n",
    "d. What happens to the log loss if we remove the feature 'equip' from the original set of input features?\n",
    "<br>\n",
    "<details><summary>Hint</summary>\n",
    "Reuse all the code statements above after modifying the value of churn_df Make sure to edit the list of features feeding the variable X. The expected answer is 0.5302427350245369\n",
    "</details>\n",
    "\n",
    "e. What happens to the log loss if we remove the features 'income' and 'employ' from the original set of input features?\n",
    "<br>\n",
    "<details><summary>Hint</summary>\n",
    "Reuse all the code statements above after modifying the value of churn_df. Make sure to edit the list of features feeding the variable X. The expected answer is 0.6529317169884828.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You're ready to move on to your next lesson!\n",
    " \n",
    " \n",
    "## Author\n",
    " \n",
    "<a href=\"https://www.linkedin.com/in/abhishek-gagneja-23051987/\" target=\"_blank\">Abishek Gagneja</a>\n",
    " \n",
    " \n",
    " ### Other Contributors\n",
    " \n",
    "<a href=\"https://www.linkedin.com/in/jpgrossman/\" target=\"_blank\">Jeff Grossman</a>\n",
    " \n",
    "<!--\n",
    "## Change Log\n",
    "\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "|2024-11-05 | 3.0 | Abhishek | Updated the descriptions, codes and lab flow | \n",
    "|2021-01-21  | 2.2  | Lakshmi  |  Updated sklearn library|\n",
    "| 2020-11-03  | 2.1  | Lakshmi  |  Updated URL of csv |\n",
    "| 2020-08-27  | 2.0  | Lavanya  |  Moved lab to course repo in GitLab |\n",
    "|   |   |   |   |\n",
    "|   |   |   |   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">© IBM Corporation. All rights reserved.</h3>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "prev_pub_hash": "c9936a3a06cf4656bb99e264a74e23001002ebb55649cddd868b26d786127876"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
